:toc:
:toclevels: 4

:sectnums:

= Science Benchmark Policy Draft: MLCommons Science Benchmark Suite Rules

Under development. 
* https://github.com/laszewsk/mlcommons/edit/main/www/content/en/docs/science_policy.adoc
* Future location
* https://github.com/mlcommons/science/blob/master/science_policy.adoc

The Policy draft document

:sectnums:


Version 0.3 
May 4, 2022

Points of contact: Gregor von Laszewski(laszewski@gmail.com), Juri Papay (juripapay@hotmail.com)

Supporting documents

We included here a list of supporting documents that will be removed
in the final version, but could be helping in shaping this draft:

* https://docs.google.com/presentation/d/1xo_M3dEV1BS7OcXjvjyOUOLkHh8WyHuawqj1OR2iJw4/edit#slide=id.g10e8f04304c_1_73[Presentation]
* https://docs.google.com/document/d/1WwcS0gjVoz5Bf0G05xKIgoh2WEBxmNQM8VmkHNP67ag/edit[Benchmarks] Is this the correct link?

== Overview

The Science WG will have training and inference benchmarks. Here we specify the rules for training submissions (the first 4 benchmarks are training but we expect inference benchmarks in the next round)

The Science WG will have Closed and Open divisions and submissions to these divisions must be separate although the same  activity could qualify for both. The Open division is expected to be the primary focus.

A Closed division submission should report system performance as the result and give the logging information outlined in MLPerf HPC Rules. The stopping criterion will be the value of loss specified in the benchmark. Power measurements may also be supplied.

An Open division submission aims to improve scientific discovery from the dataset specified in the benchmark which will specify one or more scientific measurements to be calculated in the submission. The result will be the value of these specified measurements from the submitted model. This model can be based on the supplied reference model or totally distinct. Data augmentation is allowed and all hyperparameters can be changed in the reference model if used. The result should be a GitHub (markdown) document starting with a table listing Measurement name, Reference model value, Submitted model value. For benchmarks with more than one measurement, an average difference between submitted and reference measurements should be given. Power and performance values are optional but encouraged in the results document. The result document should give enough details on the submitted model and any data augmentation that the review team can evaluate its scientific soundness. Citations should be included to describe the scientific basis of the approach. Other rules for the Open division are as described in MLCommons Training Rules (HPC has no special rules for the Open division).


All rules are taken from the
https://github.com/mlcommons/training_policies/blob/master/training_rules.adoc[MLPerf
Training Rules] except for those that are overridden here.

The MLPerf and https://mlcommons.org[MLCommons] name and logo are
trademarks. In order to refer to a result using the MLPerf and
MLCommons name, the result must conform to the letter and spirit of
the rules specified in this document. The MLCommons organization
reserves the right to solely determine if a use of its name or logo is
acceptable.

== Benchmarks

The benchmark suite consists of the benchmarks shown in the following
table.

WARNING: change the table, add Geoffrey's paper (Gregor), CloudMask, Juri - talk to Murali. What is the scientific metrics, convergence target (loss value).

|===
|Problem |Dataset |Quality Target
| Earthquake Prediction |Earthquake data from USGS. | Normalized Nashâ€“Sutcliffe model efficiency (NNSE), `0.8<NNSE<0.99`
| CloudMask | Multispectral image data from Sea and Land Surface Temperature Radiometer (SLSTR) instrument. | convergence target `0.9`
| STEMDL Classification | Convergent Beam Electron Diffraction (CBED) patterns. | The scientific metric for this problem is the top1 classification accuracy and F1-score (the higher the better). The main challenge is to predict 3D geometry from its 3 projections (2D images). Information about the best accuracy so far for this dataset can be found here: https://link.springer.com/chapter/10.1007/978-3-030-63393-6_30
| UNO |Molecular features of tumor cells across multiple data sources. | Score: `0.0054`
|===

== Divisions

There are two divisions of the Science Benchmark Suite, the Closed
division and the Open division.

=== Closed Division

The Closed division requires using the same preprocessing, model, and
training method as the reference implementation.

The closed division models are:

Internal Repository

|===
|Problem |Model
|EarthQuake  | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/earthquake
|CloudMask | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/cloudmask
|STEMDL  | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/stemdl
|CANDLE UNO  | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/uno
|===

Future MLCommons Repository

|===
|Problem |Model
|EarthQuake  | https://github.com/mlcommons/science/
|CloudMask | https://github.com/mlcommons/science/
|STEMDL  | https://github.com/mlcommons/science/
|CANDLE UNO  | https://github.com/mlcommons/science/
|===


== Data Set

=== Data State at Start of Run

Each reference implementation includes a download script or broadly
available method to acquire and verify the dataset.

The data at the start of the benchmark run should reside on a parallel
file system that is persistent (>= 1 month, not subject to eviction by
other users), can be downloaded to / accessed by the user, and can be
shared among users at the facility. Any staging to node-local disk or
memory or system burst buffer should be included in the benchmark time
measurement.

NOTE: discuss parallel. some scence benchmarks may not be parallel,

You must flush/reset the on-node caches prior to running each instance
of the benchmark. Due to practicality issues, you are not required to
reset off-node system-level caches.

NOTE: discuss what exactly an on node cache is ... is this an
application on node cache or something else.

We otherwise follow the training rule
xref:training_rules.adoc#data-state-at-start-of-run[Data State at
Start of Run] on consistency with the reference implementation
preprocessing and allowance for reformatting.

== Training Loop

=== Hyperparameters and Optimizer

CLOSED:

Allowed hyperparameter and optimizer settings are specified here. For
anything not explicitly mentioned here, submissions must match the
behavior and settings of the reference implementations.

In order to simplify the complex setup for scientific benchmarks 
we require that all parameters are included in the config file. 
We recommend a yaml format for config file.

=== Hyperparameters and Optimizer Earth Quake Prediction

|===
| Model | Name | Constraint | Definition | Reference Code 
| Earthquake | TFTTransformerepochs| `0 < value` | num_epochs | config 
| Earthquake | TFTTransformerbatch_size | `0 < value`, example: `64` | batch size to split training data into batches used to calculate model error and update model coefficients | config 
| Earthquake | TFTTransformertestvalbatch_size | `max(128,TFTTransformerbatch_size)` | this is a range between min and max for batch size | config
| Earthquake | TFTd_model | `0 < value`. Example: `160` | number of hidden layers in model | config 
| Earthquake | Tseq | `0 < value`. Example `26` | num of encoder steps. The size of sequence window, number of days included in that section of data | config 
| Earthquake |  TFTdropout_rate | `9.9 < value`. Example: `0.1`  | dropout rate : the dropout rate when training models to randomly drop nodes from a neural network to prevent overfitting | config 
| Earthquake | learning_rate | `0.0 < value`. Example: `0.0000005` | how quickly the model adapts to the problem, larger means faster convergence but less optimal solutions, slower means slower convergence but more optimal solutions potentially fail if learning rate it too small.in general a variable learning rate is best. start larger and decrease as you see less returns or as your solution converges. | config 
| Earthquake | early_stopping_patience | `0 < value`. Example: `60` |  Early stopping param for keras, a way to prevent overfit or various metric decreases | config 
|===
 
=== Hyperparameters and Optimizer CloudMask

|===
|Model |Name |Constraint |Definition |Reference Code
| CloudMask | epochs| `value > 0` | Number of epochs | config 
| CloudMask | learning_rate| `value > 0.0`. Example: `0.001` | Learning rate | config 
| CloudMask | batch_size| `value > 0`. Example: `32` | Batch size | config 
| CloudMask | MIN_SST| `value > 273.15` | Min allowable Sea Surface Temperature | config 
| CloudMask | PATCH_SIZE| `value = 256` | Size of image patches | config
| CloudMask | seed| `value = 1234` | Random seed | config
|=== 

=== Hyperparameters and Optimizer STEMDL Classification

WARNING: TBD. Next values will all be replaced with application
specific values. 

|===
| Model | Name | Constraint | Definition | Reference Code 
| STEMDL | num_epochs| `value > 0` | Number of epochs | config 
| STEMDL | learning_rate| `value > 0.0`. Example: `0.001` | Learning rate | config 
| STEMDL | batch_size| `value > 0`.Example: `32` | Batch size | config
|===

=== Hyperparameters and Optimizer CANDLE UNO

WARNING: TBD. Next values will all be replaced with application
specific values. Are the input parameters for UNO in config file?

|===
| Model | Name | Constraint | Definition | Reference Code 
| CANDLE UNO | num_epochs| `value > 0` |  Number of epochs | config 
| CANDLE UNO | learning_rate| `value > 0.0`. Example: `0.001` | Learning rate | config 
| CANDLE UNO | batch_size| `value > 0`.Example: `32` | Batch size | config
|===


OPEN: Hyperparameters and optimizer may be freely changed.

== Run Results

MLCommon Science Benchmark Suite submissions consist of the following
two metrics: metrics 1 is considered mandatory for a complete
submission whereas metric 2 is considered optional:

=== Strong Scaling (Time to Convergence)

This is a *mandatory* metric: see MLPerf Training
xref:training_rules.adoc#section-run-results[Run Results] for
reference. The same rules apply here.

=== Weak Scaling (Throughput)

At this time we are not considering weak scaling. 

== Benchmark Results

We follow MLPerf Training
xref:training_rules.adoc#benchmark-results[Benchmark Results] rule
along with the following required number of runs per benchmark.  Note
that since run-to-run variability is already captured by spatial
multiplexing in case of metric 3, we use the adjusted requirement that
the number of trained instances has to be at least equal to the number
of runs for metric 1 and 2.

The numbers given below reflect the minimum number of repetitive runs required to produce repeatable metrics.

|===
|Benchmark |Number of Runs (Metric 1, 2) | M' (Metric 3)
|Earthquake | 5 | >=5
|CloudMask | 10 | >=10
|STEMDL Classification | 5 | >=5
|CANDLE UNO | 5 | >=5
|===
