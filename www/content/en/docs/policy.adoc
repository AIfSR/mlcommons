:toc:
:toclevels: 4

:sectnums:

= Science Benchmark Policy Draft: MLCommons Science Benchmark Suite Rules

Under development.

The Policy draft document

:sectnums:


Version 0.2 
May 3, 2022

Points of contact: Gregor von Laszewski(laszewski@gmail.com), Juri Papay (juripapay@hotmail.com)

Supporting documents

We included here a list of supporting documents that will be removed
in the final version, but could be helping in shaping this draft:

* https://docs.google.com/presentation/d/1xo_M3dEV1BS7OcXjvjyOUOLkHh8WyHuawqj1OR2iJw4/edit#slide=id.g10e8f04304c_1_73[Presentation]
* https://docs.google.com/document/d/1WwcS0gjVoz5Bf0G05xKIgoh2WEBxmNQM8VmkHNP67ag/edit[Benchmarks] Is this the correct link?

== Overview

The Science WG will have training and inference benchmarks. Here we specify the rules for training submissions (the first 4 benchmarks are training but we expect inference benchmarks in the next round)

The Science WG will have Closed and Open divisions and submissions to these divisions must be separate although the same  activity could qualify for both. The Open division is expected to be the primary focus.

A Closed division submission should report system performance as the result and give the logging information outlined in MLPerf HPC Rules. The stopping criterion will be the value of loss specified in the benchmark. Power measurements may also be supplied.

An Open division submission aims to improve scientific discovery from the dataset specified in the benchmark which will specify one or more scientific measurements to be calculated in the submission. The result will be the value of these specified measurements from the submitted model. This model can be based on the supplied reference model or totally distinct. Data augmentation is allowed and all hyperparameters can be changed in the reference model if used. The result should be a GitHub (markdown) document starting with a table listing Measurement name, Reference model value, Submitted model value. For benchmarks with more than one measurement, an average difference between submitted and reference measurements should be given. Power and performance values are optional but encouraged in the results document. The result document should give enough details on the submitted model and any data augmentation that the review team can evaluate its scientific soundness. Citations should be included to describe the scientific basis of the approach. Other rules for the Open division are as described in MLCommons Training Rules (HPC has no special rules for the Open division).


All rules are taken from the
https://github.com/mlcommons/training_policies/blob/master/training_rules.adoc[MLPerf
Training Rules] except for those that are overridden here.

The MLPerf and https://mlcommons.org[MLCommons] name and logo are
trademarks. In order to refer to a result using the MLPerf and
MLCommons name, the result must conform to the letter and spirit of
the rules specified in this document. The MLCommons organization
reserves the right to solely determine if a use of its name or logo is
acceptable.

== Benchmarks

The benchmark suite consists of the benchmarks shown in the following
table.

WARNING: change the table

|===
|Problem |Dataset |Quality Target
| Earth Quake Prediction | TBD | TBD (some error minimization)
| CloudMask | TBD | TBD (some error minimization)
| STEMDL Classification | TBD | TBD (some error minimization)
| UNO | TBD | TBD (some error minimization)
|===

== Divisions

There are two divisions of the Science Benchmark Suite, the Closed
division and the Open division.

=== Closed Division

The Closed division requires using the same preprocessing, model, and
training method as the reference implementation.

The closed division models are:

|===
|Problem |Model
|REPLACE: Climate segmentation  |https://github.com/azrael417/mlperf-deepcam
|REPLACE: Cosmological parameter prediction |https://github.com/sparticlesteve/cosmoflow-benchmark
|REPLACE: Modeling catalysts |https://github.com/sparticlesteve/ocp/tree/mlperf-hpc-reference
|===

== Data Set

=== Data State at Start of Run

Each reference implementation includes a download script or broadly
available method to acquire and verify the dataset.

The data at the start of the benchmark run should reside on a parallel
file system that is persistent (>= 1 month, not subject to eviction by
other users), can be downloaded to / accessed by the user, and can be
shared among users at the facility. Any staging to node-local disk or
memory or system burst buffer should be included in the benchmark time
measurement.

NOTE: discuss parallel. some scence benchmarks may not be parallel,

You must flush/reset the on-node caches prior to running each instance
of the benchmark. Due to practicality issues, you are not required to
reset off-node system-level caches.

NOTE: discuss what exactly an on node cache is ... is this an
application on node cache or something else.

We otherwise follow the training rule
xref:training_rules.adoc#data-state-at-start-of-run[Data State at
Start of Run] on consistency with the reference implementation
preprocessing and allowance for reformatting.

== Training Loop

=== Hyperparameters and Optimizer

CLOSED:

Allowed hyperparameter and optimizer settings are specified here. For
anything not explicitly mentioned here, submissions must match the
behavior and settings of the reference implementations.

=== Hyperparameters and Optimizer Earth Quake Prediction

WARNING: TBD. Next values will all be replaced with application
specific values

|===
| Model | Name | Constraint | Definition | Reference Code 
| Earthquake | TFTTransformerepochs| `0 < value` | num_epochs | config 
| Earthquake | TFTTransformerbatch_size | `0 < value`, example: `64` | batch size to split training data into batches used to calculate model error and update model coefficients | config 
| Earthquake | TFTTransformertestvalbatch_size | `max(128,TFTTransformerbatch_size)` | this is a range between min and max for batch size | config
| Earthquake | TFTd_model | `0 < value`. Example: `160` | number of hidden layers in model | config 
| Earthquake | Tseq | `0 < value`. Example `26` | num of encoder steps. The size of sequence window, number of days included in that section of data | config 
| Earthquake |  TFTdropout_rate | `9.9 < value`. Example: `0.1`  | dropout rate : the dropout rate when training models to randomly drop nodes from a neural network to prevent overfitting | config 
| Earthquake | learning_rate | `0.0 < value`. Example: `0.0000005` | how quickly the model adapts to the problem, larger means faster convergence but less optimal solutions, slower means slower convergence but more optimal solutions potentially fail if learning rate it too small.in general a variable learning rate is best. start larger and decrease as you see less returns or as your solution converges. | config 
| Earthquake | early_stopping_patience | `0 < value`. Example: `60` |  Early stopping param for keras, a way to prevent overfit or various metric decreases | config 
|===
 
=== Hyperparameters and Optimizer CloudMask

WARNING: TBD. Next values will all be replaced with application specific values
 
|===
|Model |Name |Constraint |Definition |Reference Code
| CloudMask | epochs| `value > 0` | num_epochs | config 
| CloudMask | learning_rate| `value > 0.0`. Example: `0.001` | learning_rate | config 
| CloudMask | batch_size| `value > 0`. Example: `32` | batch_size | config 
| CloudMask | MIN_SST| `value > 273.15` | Min allowable Sea Surface Temperature | config 
| CloudMask | PATCH_SIZE| `value = 256` | Size of image patches | config
| CloudMask | IMAGE_H| `value = 1200` | Image height | config
| CloudMask | IMAGE_W| `value = 1500` | Image width | config
| CloudMask | N_CHANNELS| `value = 9` | Number of channels | config
| CloudMask | CROP_SIZE| `value = 80` | Amount to crop the edges of the images by | config
| CloudMask | clip_offset| `value = 15` | Clip offset | config
| CloudMask | seed| `value = 1234` | Random seed | config
| CloudMask | train_split| `value = 0.8` | Ratio of splitting the dataset into training and testing data | config
|=== 

=== Hyperparameters and Optimizer STEMDL Classification

WARNING: TBD. Next values will all be replaced with application
specific values

|===
| Model | Name | Constraint | Definition | Reference Code 
| STEMDL | num_epochs| `value > 0` | num_epochs | config 
| STEMDL | learning_rate| `value > 0.0`. Example: `0.001` | learning_rate | config 
| STEMDL | batch_size| `value > 0`.Example: `32` | Batch size | config
| STEMDL | train_split| `value = 0.8` | Ratio of splitting the dataset into training and testing data | config
|===

OPEN: Hyperparameters and optimizer may be freely changed.

== Run Results

MLCommon Science Benchmark Suite submissions consist of the following
two metrics: metrics 1 is considered mandatory for a complete
submission whereas metric 2 is considered optional:

=== Strong Scaling (Time to Convergence)

This is a *mandatory* metric: see MLPerf Training
xref:training_rules.adoc#section-run-results[Run Results] for
reference. The same rules apply here.

=== Weak Scaling (Throughput)

TODO 

This is an *optional* metric. It was designed to test the training
capacity of a system.

Measurement: we will define 3 important parameters first. 

* number of models M: number of model instances which are going to be
  trained in this benchmark.
* instance scale S: each individual model instance will be trained at
  this scale.
* total utilized scale T: the total scale used for running this
  benchmark. For example, if all M models are trained concurrently,
  then T=M*S. More generally we can write that S<=T<=M*S if (some of)
  the models are trained sequentially.

Notes:

* All three numbers M,S,T are chosen by the submitter. This allows the
  submitter to accomodate their submission to available machine
  resources, i.e. compute capacity and compute time.
* S and T should be in units of compute resources, e.g. nodes, GPUs or
  other accelerators. This choice should be aligned with the HPC
  system description. For example, if the systems descriptions table
  lists number GPUs to define the scale of the system, then S should
  be specified in numbers of GPUs.
* S and T can be chosen independently of the submission for metric 1
  (strong scaling). We encourage to choose T as large as possible,
  ideally full system scale, but this is not required.

The submitter then trains M models on the resource partitioning (S,T)
as defined above to convergence.

We define a Time-To-Train-all (TTTa) number by computing the
difference between the end time of the instance which needs longest
time to converge and the start time of the instance which starts up
fastest. Mathematically this can be expressed as

----
TTTa = max(run_stop) - min(run_start) where the max/min are taken over all instances M. 
----

Note: the submitter is allowed to prune this number by removing
results from individual training instances. As long as the minimum
number of models rule is satisfied (see section <<Benchmark Results>>
below), the submission is valid. They then use a modified number of
models M'<=M and computes TTTa over the reduced set. This allows the
submitter to remove occasional outliers or stragglers which would
otherwise reduce the score disproportionally.

Reporting: the submitter reports the the tuple (T, S, M', TTTa).  It
is required to submit a separate MLLOG file for each of the training
instances, so that reviewers can verify the quoted numbers.  It is not
allowed to merge logging files for individual instances.

Restrictions: 

* The submitter *must not report this score on its own*. It has to be
  reported in conjunction with at least one score from <<Strong
  Scaling (Time to Convergence)>> from the same benchmark.
* this score *does not allow for extrapolation*. All reported M'
  training instances must have converged and it is not allowed to
  extrapolate results in S or T.


== Benchmark Results

We follow MLPerf Training
xref:training_rules.adoc#benchmark-results[Benchmark Results] rule
along with the following required number of runs per benchmark.  Note
that since run-to-run variability is already captured by spatial
multiplexing in case of metric 3, we use the adjusted requirement that
the number of trained instances has to be at least equal to the number
of runs for metric 1 and 2.

WARNING: TBD. Next values will all be replaced with application specific values

|===
|Benchmark |Number of Runs (Metric 1, 2) | M' (Metric 3)
|DeepCAM | 5 | >=5
|CosmoFlow | 10 | >=10
|OpenCatalyst | 5 | >=5
|===
