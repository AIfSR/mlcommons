:toc:
:toclevels: 4

:sectnums:

= Science Benchmark Policy: MLCommons® Science Benchmark Suite Rules



* https://github.com/laszewsk/mlcommons/edit/main/www/content/en/docs/science_policy.adoc

:sectnums:


Version 0.4
July 23, 2022

Points of contact:

* Geoffrey Fox(gcfexchange@gmail.com)
* Tony Hey (Tony.Hey@stfc.ac.uk)
* Gregor von Laszewski(laszewski@gmail.com)
* Juri Papay (juripapay@hotmail.com)


== Overview

The Science WG will have training and inference benchmarks. Here we
specify the rules for training submissions.

The Science WG will have Closed and Open divisions and submissions to
these divisions must be separate although the same activity could
qualify for both. The Open division is expected to be the primary
focus.

A Closed division submission should report system performance as the
result and give the logging information outlined in MLPerf HPC
Rules. The stopping criterion will be the value of loss specified in
the benchmark. Power measurements may also be supplied.

An Open division submission aims to improve scientific discovery from
the dataset specified in the benchmark which will specify one or more
scientific measurements to be calculated in the submission. The result
will be the value of these specified measurements from the submitted
model. This model can be based on the supplied reference model or
distinct. Data augmentation is allowed and all hyperparameters
can be changed in the reference model if used. The result should be a
GitHub (markdown) document starting with a table listing *Measurement
name*, *Reference model value*, *Submitted model value*. For benchmarks
with more than one measurement, an average difference between submitted
and reference measurements should be given. Power and
performance values are optional but encouraged in the results
document. The resulting document should give enough details on the
submitted model and any data augmentation so that the review team can
evaluate its scientific soundness. Citations should be included to
describe the scientific basis of the approach. Other rules for the
Open division are as described in MLCommons® Training Rules (HPC has
no special rules for the Open division).



All rules are taken from the
https://github.com/mlcommons/training_policies/blob/master/training_rules.adoc[MLPerf
Training Rules] except for those that are overridden here.

The MLPerf and https://mlcommons.org[MLCommons®] name and logo are
trademarks. In order to refer to a result using the MLPerf and
MLCommons® name, the result must conform to the letter and spirit of
the rules specified in this document. The MLCommons® organization
reserves the right to solely determine if a use of its name or logo is
acceptable.

== Benchmarks

The benchmark suite consists of the benchmarks shown in the following
table.

WARNING: change the table, add Geoffrey's paper (Gregor), CloudMask, Juri - talk to Murali. What is the scientific metrics, convergence target (loss value).

|===
|Problem |Dataset |Quality Target
| Earthquake Prediction |Earthquake data from USGS. | Normalized Nash–Sutcliffe model efficiency (NNSE), `0.8<NNSE<0.99`, Details can be found in [3].
| CloudMask | Multispectral image data from Sea and Land Surface Temperature Radiometer (SLSTR) instrument. | convergence target `0.9`
| STEMDL Classification | Convergent Beam Electron Diffraction (CBED) patterns. | The scientific metric for this problem is the top1 classification accuracy and F1-score (the higher the better). The main challenge is to predict 3D geometry from its 3 projections (2D images). Information about the best accuracy so far for this dataset can be found in [4]
| UNO |Molecular features of tumor cells across multiple data sources. | Score: `0.0054`
|===

== Divisions

There are two divisions of the Science Benchmark Suite, the Closed
division and the Open division.

=== Closed Division

The Closed division requires using the same preprocessing, model, and
training method as the reference implementation.

The closed division models are:

Internal Repository

|===
|Problem |Model
|EarthQuake  | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/earthquake/latest
|CloudMask | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/cloudmask
|STEMDL  | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/stemdl
|CANDLE UNO  | https://github.com/laszewsk/mlcommons/tree/main/benchmarks/uno
|===

Future MLCommons® Repository

|===
|Problem |Model
|EarthQuake  | https://github.com/mlcommons/science/
|CloudMask | https://github.com/mlcommons/science/
|STEMDL  | https://github.com/mlcommons/science/
|CANDLE UNO  | https://github.com/mlcommons/science/
|===


== Data Set

=== Data State at Start of Run

Each reference implementation includes a download script or broadly
available method to acquire and verify the dataset.

The data at the start of the benchmark run should reside on a parallel
file system that is persistent (>= 1 month, not subject to eviction by
other users), can be downloaded to / accessed by the user, and can be
shared among users at the facility. Any staging to node-local disk or
memory or system burst buffer should be included in the benchmark time
measurement.

NOTE: discuss parallel. some scence benchmarks may not be parallel,

You must flush/reset the on-node caches prior to running each instance
of the benchmark. Due to practicality issues, you are not required to
reset off-node system-level caches.

NOTE: discuss what exactly an on node cache is ... is this an
application on node cache or something else.

We otherwise follow the training rule
xref:training_rules.adoc#data-state-at-start-of-run[Data State at
Start of Run] on consistency with the reference implementation
preprocessing and allowance for reformatting.

== Training Loop

=== Hyperparameters and Optimizer

CLOSED:

Allowed hyperparameter and optimizer settings are specified here. For
anything not explicitly mentioned here, submissions must match the
behavior and settings of the reference implementations.

In order to simplify the complex setup for scientific benchmarks 
we require that all parameters are included in the config file. 
We recommend a yaml format for config file.

=== Hyperparameters and Optimizer Earth Quake Prediction

|===
| Model | Name | Constraint | Definition | Reference Code 
| Earthquake | TFTTransformerepochs| `0 < value` | num_epochs | config 
| Earthquake | TFTTransformerbatch_size | `0 < value`, example: `64` | batch size to split training data into batches used to calculate model error and update model coefficients | config 
| Earthquake | TFTTransformertestvalbatch_size | `max(128,TFTTransformerbatch_size)` | this is a range between min and max for batch size | config
| Earthquake | TFTd_model | `0 < value`. Example: `160` | number of hidden layers in model | config 
| Earthquake | Tseq | `0 < value`. Example `26` | num of encoder steps. The size of sequence window, number of days included in that section of data | config 
| Earthquake |  TFTdropout_rate | `9.9 < value`. Example: `0.1`  | dropout rate : the dropout rate when training models to randomly drop nodes from a neural network to prevent overfitting | config 
| Earthquake | learning_rate | `0.0 < value`. Example: `0.0000005` | how quickly the model adapts to the problem, larger means faster convergence but less optimal solutions, slower means slower convergence but more optimal solutions potentially fail if learning rate it too small.in general a variable learning rate is best. start larger and decrease as you see less returns or as your solution converges. | config 
| Earthquake | early_stopping_patience | `0 < value`. Example: `60` |  Early stopping param for keras, a way to prevent overfit or various metric decreases | config 
|===
 
=== Hyperparameters and Optimizer CloudMask

|===
|Model |Name |Constraint |Definition |Reference Code
| CloudMask | epochs| `value > 0` | Number of epochs | config 
| CloudMask | learning_rate| `value > 0.0`. Example: `0.001` | Learning rate | config 
| CloudMask | batch_size| `value > 0`. Example: `32` | Batch size | config 
| CloudMask | MIN_SST| `value > 273.15` | Min allowable Sea Surface Temperature | config 
| CloudMask | PATCH_SIZE| `value = 256` | Size of image patches | config
| CloudMask | seed| `value = 1234` | Random seed | config
|=== 

=== Hyperparameters and Optimizer STEMDL Classification

WARNING: TBD. Next values will all be replaced with application
specific values. 

|===
| Model | Name | Constraint | Definition | Reference Code 
| STEMDL | num_epochs| `value > 0` | Number of epochs | config 
| STEMDL | learning_rate| `value > 0.0`. Example: `0.001` | Learning rate | config 
| STEMDL | batch_size| `value > 0`.Example: `32` | Batch size | config
|===

=== Hyperparameters and Optimizer CANDLE UNO

WARNING: TBD. Next values will all be replaced with application
specific values. Are the input parameters for UNO in config file?

|===
| Model | Name | Constraint | Definition | Reference Code 
| CANDLE UNO | num_epochs| `value > 0` |  Number of epochs | config 
| CANDLE UNO | learning_rate| `value > 0.0`. Example: `0.001` | Learning rate | config 
| CANDLE UNO | batch_size| `value > 0`.Example: `32` | Batch size | config
|===


OPEN: Hyperparameters and optimizer may be freely changed.

== Run Results

MLCommon® Science Benchmark Suite submissions consist of the following
two metrics: metrics 1 is considered mandatory for a complete
submission whereas metric 2 is considered optional:

=== Strong Scaling (Time to Convergence)

This is a *mandatory* metric: see MLPerf Training
xref:training_rules.adoc#section-run-results[Run Results] for
reference. The same rules apply here.

=== Weak Scaling (Throughput)

At this time we are not considering weak scaling. 

== Benchmark Results

We follow MLPerf Training
Benchmark Results rule
along with the following required number of runs per benchmark.  Note
that since run-to-run variability is already captured by spatial
multiplexing in case of metric 3, we use the adjusted requirement that
the number of trained instances has to be at least equal to the number
of runs for metric 1 and 2.

The numbers given below reflect the minimum number of repetitive runs
required to produce repeatable metrics.
In case of the Earthuake benchmark we have reduced the number of runs to 1 for metric 1, as the runs take a long time (between 5 - 12h on NVidia GPUs).

|===
| |Number of Runs |Number of Runs |Number of Runs
|Benchmark | Metric 1 |  Metric 2 | Metric 3
|Earthquake | 1 | 5 | >=5
|CloudMask | 10 | 10 | >=10
|STEMDL Classification | 5 | 5 | >=5
|CANDLE UNO | 5 | 5 | >=5
|===

== Contributing

We expect that over time additional benchmarks will be contributed. At this time we have adopted the following best practice for the contribution workflow

1. The initial benchmark is hosted on a group accessible GitHub repository, where memberes have full access rights. This may be different repositories. Currently, we have one repository at https://github.com/laszewsk/mlcommons[[10]].
2. New version will first be made available in that group repository while using branching.
3. A new candidate version is created and merged into main.
4. The candidate version is internally tested by the group members to evaluate expected behaviour.
5. Once passsed, the code is uploaded to the  https://github.com/mlcommons/science[MLCommons® Science GitHub Repository [9]].
6. Announcements are made to solicit submissions.
7. Submissions are checked and integrated according to the MLCommons® rules and policies.

[bibliography]
== References

We included here a list of supporting and related documents

* [1] https://github.com/laszewsk/mlcommons/raw/main/pub/Science-WG-of-MLCommons-presentation.pdf[Overview presentation of the MLScience Group]  Barrett,
Wahid Bhimji,
Bala Desinghu,
Murali Emani,
Geoffrey Fox,
Grigori Fursin,
Tony Hey,
David Kanter,
Christine Kirkpatrick,Hai Ah Nam,
Juri Papay,
Amit  Ruhela,
Mallikarjun Shankar,
Jeyan Thiyagalingam
Aristeidis Tsaris,
Gregor von Laszewski,
Feiyi Wang,
Junqi Yin
, MLCommons® Community Meeting, (also available in
https://docs.google.com/presentation/d/1xo_M3dEV1BS7OcXjvjyOUOLkHh8WyHuawqj1OR2iJw4/edit#slide=id.g10e8f04304c_1_73[Google docs]), December 9 2021.

* [2] https://github.com/laszewsk/mlcommons/raw/main/pub/mlcommons_science_wg_paper_2022.pdf[AI Benchmarking for Science: Efforts from the
MLCommons® Science Working Group], Jeyan Thiyagalingam, Gregor von Laszewski, Junqi Yin, Murali Emani,
Juri Papay, Gregg Barrett, Piotr Luszczek, Aristeidis Tsaris,
Christine Kirkpatrick, Feiyi Wang, Tom Gibbs, Venkatram Vishwanath,
Mallikarjun Shankar, Geoffrey Fox, Tony Hey, June 2022

* [3] https://mdpi-res.com/d_attachment/geohazards/geohazards-03-00011/article_deploy/geohazards-03-00011-v2.pdf?version=1650104721[Earthquake Nowcasting with Deep
Learning], Fox, G., Rundle, J., Donnellan, A., Feng, B., Geohazards 3(2), 199, April 2022

* [4] https://doi.org/10.1007/978-3-030-63393-6_30[Probability Flow for Classifying Crystallographic Space Groups] Pan, J.,  In: Nichols, J., Verastegui, B., Maccabe, A.‘., Hernandez, O., Parete-Koon, S., Ahearn, T. (eds) Driving Scientific and Engineering Discoveries Through the Convergence of HPC, Big Data and AI. SMC 2020. Communications in Computer and Information Science, vol 1315. Springer, Cham., 2022


* [5] https://mlcommons.org/en/policies/[MLCommons® Policies]

* [6] https://github.com/mlcommons/training_policies[MLCommons® Training policies]

* [7] https://github.com/mlcommons/inference_policies[MLCommons® Interference Policies]

* [8] https://github.com/mlcommons/policies[MLCommons® submission Rules for training and inference]

* [9] https://github.com/mlcommons/science[MLCommons® Science GitHub Repository]

* [10] https://github.com/laszewsk/mlcommons[Science Development GitHub Repository to prepare release candidates for the MLCommons repository]